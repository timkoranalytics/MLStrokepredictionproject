---
title: "ML_PROJECT"
output: html_document
date: "2025-04-17"
---

```{r}
library(tidyverse)
test <- read.csv("healthcare-dataset-stroke-data.csv")
test$bmi <- as.numeric(test$bmi)
test$bmi[is.na(test$bmi)] <- median(test$bmi, na.rm = TRUE)

stroke_data1 <- read.csv("healthcare-dataset-stroke-data.csv")
  
#Varaibles transformation
stroke_data1$bmi <- as.numeric(stroke_data1$bmi)
stroke_data1$bmi[is.na(stroke_data1$bmi)] <- median(stroke_data1$bmi, na.rm = TRUE)
stroke_data1$smoking_status <- na_if(stroke_data1$smoking_status, "Unknown")
stroke_data1$stroke <- as.numeric(stroke_data1$stroke)


stroke_data1 <- stroke_data1 %>%
  select(-id) %>%
  filter(!is.na(smoking_status)) %>% 
  filter(gender != "Other") 
  

sum(is.na(stroke_data1$smoking_status))
sum(is.na(stroke_data1$bmi))


```


```{r}
packages <- c("tidyverse", "ggplot2")
install.packages(setdiff(packages, installed.packages()[, "Package"]))
lapply(packages, library, character.only = TRUE)

```


```{r}
library(tidyverse)
ggplot(stroke_data1, aes(x = gender, fill = factor(stroke))) +
  geom_bar(position = "fill", color = "black") +
  scale_fill_manual(values = c("0" = "firebrick", "1" = "seagreen"),
                    labels = c("0" = "No Stroke", "1" = "Stroke")) +
  labs(title = "Stroke Distribution by Gender",
       x = "Gender", y = "Count", fill = "Stroke") +
  theme_minimal()
```
```{r}
ggplot(stroke_data1, aes(x = factor(hypertension), fill = factor(stroke))) +
  geom_bar(position = "fill", color = "black") +
  scale_fill_manual(values = c("0" = "firebrick", "1" = "seagreen"),
                    labels = c("0" = "No Stroke", "1" = "Stroke")) +
  labs(title = "Stroke Distribution by Hypertension",
       x = "Hypertension (0 = No, 1 = Yes)", y = "Proportion", fill = "Stroke") +
  theme_minimal() +
  facet_wrap(~gender)

```
```{r}
ggplot(stroke_data1, aes(x = factor(heart_disease), fill = factor(stroke))) +
  geom_bar(position = "fill", color = "black") +
  scale_fill_manual(values = c("0" = "firebrick", "1" = "seagreen"),
                    labels = c("0" = "No Stroke", "1" = "Stroke")) +
  labs(title = "Stroke Distribution by Heart Disease",
       x = "Heart Disease (0 = No, 1 = Yes)", y = "Proportion", fill = "Stroke") +
  theme_minimal() +
  facet_wrap(~gender)

```
```{r}
ggplot(stroke_data1, aes(x = ever_married, fill = factor(stroke))) +
  geom_bar(position = "fill", color = "black") +
  scale_fill_manual(values = c("0" = "firebrick", "1" = "seagreen"),
                    labels = c("0" = "No Stroke", "1" = "Stroke")) +
  labs(title = "Stroke Distribution by Marital Status",
       x = "Ever Married", y = "Proportion", fill = "Stroke") +
  theme_minimal() +
  facet_wrap(~gender)
```
```{r}
ggplot(stroke_data1, aes(x = work_type, fill = factor(stroke))) +
  geom_bar(position = "fill", color = "black") +
  scale_fill_manual(values = c("0" = "firebrick", "1" = "seagreen"),
                    labels = c("0" = "No Stroke", "1" = "Stroke")) +
  labs(title = "Stroke Distribution by Work Type",
       x = "Work Type", y = "Proportion", fill = "Stroke") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) 

```
```{r}
ggplot(stroke_data1, aes(x = Residence_type, fill = factor(stroke))) +
  geom_bar(position = "fill", color = "black") +
  scale_fill_manual(values = c("0" = "firebrick", "1" = "seagreen"),
                    labels = c("0" = "No Stroke", "1" = "Stroke")) +
  labs(title = "Stroke Distribution by Residence Type",
       x = "Residence Type", y = "Proportion", fill = "Stroke") +
  theme_minimal() +
  facet_wrap(~gender)
```
```{r}
ggplot(stroke_data1, aes(x = smoking_status, fill = factor(stroke))) +
  geom_bar(position = "fill", color = "black") +
  scale_fill_manual(values = c("0" = "firebrick", "1" = "seagreen"),
                    labels = c("0" = "No Stroke", "1" = "Stroke")) +
  labs(title = "Stroke Distribution by Smoking Status",
       x = "Smoking Status", y = "Proportion", fill = "Stroke") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) +
  facet_wrap(~gender)
```
```{r}
ggplot(stroke_data1, aes(x = age)) +
  geom_histogram(fill = "skyblue", color = "black", bins = 30) +
  labs(title = "Age Distribution", x = "Age", y = "Count") +
  theme_minimal()

ggplot(stroke_data1, aes(x = factor(stroke), y = age, fill = factor(stroke))) +
  geom_boxplot() +
  scale_fill_manual(values = c("firebrick", "seagreen")) +
  labs(title = "Age by Stroke Group", x = "Stroke", y = "Age") +
  theme_minimal() +
  facet_wrap(~gender)


```
```{r}
ggplot(stroke_data1, aes(x = bmi)) +
  geom_histogram(fill = "darkblue", color = "black", bins = 30) +
  labs(title = "Age Distribution", x = "bmi", y = "Count") +
  theme_minimal()

ggplot(stroke_data1, aes(x = factor(stroke), y = bmi, fill = factor(stroke))) +
  geom_boxplot() +
  scale_fill_manual(values = c("firebrick", "seagreen")) +
  labs(title = "BMI by Stroke Group", x = "Stroke", y = "BMI") +
  theme_minimal() +
  facet_wrap(~gender)

```
```{r}
ggplot(stroke_data1, aes(x = avg_glucose_level)) +
  geom_histogram(fill = "purple", color = "black", bins = 30) +
  labs(title = "Avarage Glucose level Distribution", x = "Glucose_level", y = "Count") +
  theme_minimal()

ggplot(stroke_data1, aes(x = factor(stroke), y = avg_glucose_level, fill = factor(stroke))) +
  geom_boxplot() +
  scale_fill_manual(values = c("firebrick", "seagreen")) +
  labs(title = "Avarage Glucose level by Stroke Group", x = "Glucose_level", y = "BMI") +
  theme_minimal() +
  facet_wrap(~gender)
```

Data prep for model building and correlation testing 

```{r}
# 0. Outliers detection and removal 
#q1 <-quantile(stroke_data1$avg_glucose_level, probs=c(.25, .75), na.rm = FALSE)
#iqr1 <- IQR(stroke_data1$avg_glucose_level)
#up1 <- q1[2] + 1.5*iqr1
#low1 <- q1[1] - 1.5*iqr1
#stroke_data1 <- subset(stroke_data1, stroke_data1$avg_glucose_level > low1 & stroke_data1$avg_glucose_level < up1)

boxplot(test$avg_glucose_level, main='Body Mass Index Distribution')
boxplot(stroke_data1$avg_glucose_level, main='Body Mass Index Distribution_OutlierRemoved')
```

```{r}
#q2 <-quantile(stroke_data1$bmi, probs=c(.25, .75), na.rm = FALSE)
#iqr2 <- IQR(stroke_data1$bmi)
#up2 <- q2[2] + 1.5*iqr2
#low2 <- q2[1] - 1.5*iqr2
#stroke_data1 <- subset(stroke_data1, stroke_data1$bmi > low2 & stroke_data1$bmi < up2)

boxplot(test$bmi, main='Body Mass Index Distribution')
boxplot(stroke_data1$bmi, main='Body Mass Index Distribution with Removed Outliers')
```

```{r}
install.packages("fastDummies")
library(corrplot)
library(dplyr)
library(scales)
library(fastDummies)


# 1. One-hot encoding (turning categorial variables into numeric for future model building)

stroke_data2 <- stroke_data1 %>%
  mutate(
    # gender: we encode it as: Female = 0, Male = 1
    gender = ifelse(gender == "Male", 1, 0),
    
    # ever_married: we encode it as: No = 0, Yes = 1
    ever_married_Yes = ifelse(ever_married == "Yes", 1, 0),
    
    # Residence_type: we encode it as: Urban = 1, Rural = 0
    residence_type = ifelse(Residence_type == "Urban", 1, 0),
    
    # smoking_status: we trasnform it into three binary variables 
    smoke_formerly = ifelse(smoking_status == "formerly smoked", 1, 0),
    smoke_never = ifelse(smoking_status == "never smoked", 1, 0),
    smoke_current = ifelse(smoking_status == "smokes", 1, 0),
    
    # work_type: we transform it into 5 binary variables 
    work_private = ifelse(work_type == "Private", 1, 0),
    work_self = ifelse(work_type == "Self-employed", 1, 0),
    work_govt = ifelse(work_type == "Govt_job", 1, 0),
    work_children = ifelse(work_type == "children", 1, 0),
    work_never = ifelse(work_type == "Never_worked", 1, 0)
  ) %>% 
    select(-work_type, -Residence_type, -smoking_status, -ever_married)  #since this columns still present in our df we should drop it 

#correlation matrix building 
  
stroke_data_cor <- stroke_data2 %>% cor(method = "pearson", use = "pairwise.complete.obs")
stroke_data_cor[, "stroke"]

corrplot(stroke_data_cor, method = "number", number.digits = 1, number.cex = 0.5, tl.cex = 0.5)


#Data normalization (normalizing all the numerical variables)

stroke_data3 <- stroke_data2 %>%
  select(age, hypertension, avg_glucose_level, work_self, stroke) %>%
  mutate(
    age = rescale(age, to = c(0, 1)),
    hypertension = rescale(as.numeric(hypertension), to = c(0, 1)),
    avg_glucose_level = rescale(avg_glucose_level, to = c(0, 1)),
    work_self = rescale(as.numeric(work_self), to = c(0, 1)),
    stroke = as.factor(stroke)
  )

```
\subsection{Default Logistic regression:}

```{r}
install.packages(c("tidymodels", "themis", "glmnet", "yardstick"))
install.packages("pROC")
install.packages("ROSE")
install.packages("caret")

library(tidymodels)
library(themis)
library(glmnet)
library(yardstick)
library(ROSE)
library(pROC)
library(caret)

# Splitting our data into train and test sample 

set.seed(123)
n <- nrow(stroke_data3)
train_index <- sample(1:n, size = 0.8 * n)

train_data_2 <- stroke_data3[train_index, ]
test_data_2 <- stroke_data3[-train_index, ]

# Using glm() function to build logistic regression
model <- glm(stroke ~ ., data = train_data_2, family = binomial)

# Predictions on the test sample 
prob_predictions <- predict(model, newdata = test_data_2, type = "response")

# Trasnfering probabilities into classes 
class_predictions <- ifelse(prob_predictions > 0.5, 1, 0)
class_predictions <- factor(class_predictions, levels = c(0, 1))
test_data_2$stroke <- factor(test_data_2$stroke, levels = c(0, 1))

# Confusion matrix 
conf_matrix_default <- confusionMatrix(as.factor(class_predictions), as.factor(test_data_2$stroke))
print(conf_matrix_default)

# Roc curve 
roc_curve_def <- roc(test_data_2$stroke, prob_predictions)
auc_value_def <- auc(roc_curve_def)
print(paste("AUC: ", auc_value_def))
plot(roc_curve_def, main = "ROC Curve for Default Model", col = "blue", lwd = 2)

# Extraction of TP, FP, TN and FN from the confusion matrix 
TP <- conf_matrix_default$table[2, 2]  # True Positives
FP <- conf_matrix_default$table[1, 2]  # False Positives
TN <- conf_matrix_default$table[1, 1]  # True Negatives
FN <- conf_matrix_default$table[2, 1]  # False Negatives

# Calculating Precision, Recall, F1
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
f1_score <- 2 * (precision * recall) / (precision + recall)

# Results 
cat("Precision: ", precision, "\n")
cat("Recall: ", recall, "\n")
cat("F1: ", f1_score, "\n")

```

\subsection{Logistic regression with Oversampling technique}: 

```{r}
library(tidymodels)
library(themis)
library(glmnet)
library(yardstick)
library(ROSE)
library(pROC)
library(caret)

# Splitting our data again into test and train samples

set.seed(123)
n <- nrow(stroke_data3)
train_index <- sample(1:n, size = 0.8 * n)

train_data_3 <- stroke_data3[train_index, ]
test_data_3 <- stroke_data3[-train_index, ]

# Balancing our data with oversampling method 
balanced_data <- ovun.sample(stroke ~ ., 
                             data = train_data_3, 
                             method = "over", 
                             N = 2 * table(train_data_3$stroke)[1])$data

# Building model on the balanced data
model_balanced <- glm(stroke ~ ., 
                      data = balanced_data, 
                      family = binomial)

# Predictions on the test sample 
prob_predictions_balanced <- predict(model_balanced, 
                                     newdata = test_data_3, 
                                     type = "response")

# Transfering probabilities into classes
class_predictions_balanced <- ifelse(prob_predictions_balanced > 0.5, 1, 0)


# Predictions on the train sample
prob_predictions_train <- predict(model_balanced, 
                                  newdata = balanced_data, 
                                  type = "response")

class_predictions_train <- ifelse(prob_predictions_train > 0.5, 1, 0)

# Confusion matrix ‚Äî Train
conf_matrix_train <- confusionMatrix(as.factor(class_predictions_train), 
                                     as.factor(balanced_data$stroke))
conf_matrix_train

# ROC curve ‚Äî Train
roc_curve_train <- roc(balanced_data$stroke, prob_predictions_train)
auc_value_train <- auc(roc_curve_train)
print(paste("AUC (Train):", auc_value_train))

# Plot ROC for Train
plot(roc_curve_train, 
     main = "ROC Curve for Training Set", 
     col = "darkgreen", 
     lwd = 2)

# Train metrics
TP2 <- conf_matrix_train$table[2, 2]
FP2 <- conf_matrix_train$table[1, 2]
TN2 <- conf_matrix_train$table[1, 1]
FN2 <- conf_matrix_train$table[2, 1]

precision2 <- TP2 / (TP2 + FP2)
recall2 <- TP2 / (TP2 + FN2)
f1_score2 <- 2 * (precision2 * recall2) / (precision2 + recall2)

cat("TRAIN Precision: ", precision2, "\n")
cat("TRAIN Recall: ", recall2, "\n")
cat("TRAIN F1: ", f1_score2, "\n")

# Confusion matrix for the test sample 
conf_matrix_balanced <- confusionMatrix(as.factor(class_predictions_balanced), 
                                        as.factor(test_data_3$stroke))
conf_matrix_balanced

# AUC 
roc_curve <- roc(test_data_3$stroke, 
                 prob_predictions_balanced)

auc_value <- auc(roc_curve)
print(paste("AUC: ", auc_value))

# Metrics for the test samle 
TP1 <- conf_matrix_balanced$table[2, 2]  # True Positives
FP1 <- conf_matrix_balanced$table[1, 2]  # False Positives
TN1 <- conf_matrix_balanced$table[1, 1]  # True Negatives
FN1 <- conf_matrix_balanced$table[2, 1]  # False Negatives

# Calculating Precision, Recall, F1
precision1 <- TP1 / (TP1 + FP1)
recall1 <- TP1 / (TP1 + FN1)
f1_score1 <- 2 * (precision1 * recall1) / (precision1 + recall1)

# Results
cat("TEST Precision: ", precision1, "\n")
cat("TEST Recall: ", recall1, "\n")
cat("TEST F1: ", f1_score1, "\n")
```

```{r}
library(tidymodels)
library(themis)
library(glmnet)
library(yardstick)
library(caret)
library(pROC)

# –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ
set.seed(123)
n <- nrow(stroke_data3)
train_index <- sample(1:n, size = 0.8 * n)

train_data_5 <- stroke_data3[train_index, ]
test_data_5 <- stroke_data3[-train_index, ]

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Ä–µ—Ü–µ–ø—Ç—ã
rec <- recipe(stroke ~ ., data = train_data_5) %>%
  step_smote(stroke, over_ratio = 1)  # –ü—Ä–∏–º–µ–Ω—è–µ–º SMOTE

# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–µ—Ü–µ–ø—Ç –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
train_data_smote <- prep(rec, training = train_data_5) %>% bake(new_data = NULL)

# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π –Ω–∞ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
model_smote <- glm(stroke ~ ., 
                   data = train_data_smote,
                   family = binomial)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
prob_predictions_smote <- predict(model_smote, 
                                  newdata = test_data_5, 
                                  type = "response")

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤ –∫–ª–∞—Å—Å—ã —Å –ø–æ—Ä–æ–≥–æ–º 0.75
class_predictions_smote <- ifelse(prob_predictions_smote > 0.75, 1, 0)

# –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –¥–ª—è –º–æ–¥–µ–ª–∏ —Å SMOTE
conf_matrix_smote <- confusionMatrix(as.factor(class_predictions_smote), 
                                     as.factor(test_data_5$stroke),
                                      positive = "1")

print(conf_matrix_smote)

# ROC –∫—Ä–∏–≤–∞—è
roc_curve_smote <- roc(test_data_5$stroke, prob_predictions_smote)
auc_value_smote <- auc(roc_curve_smote)
cat("AUC –¥–ª—è –º–æ–¥–µ–ª–∏ —Å SMOTE: ", auc_value_smote, "\n")
plot(roc_curve_smote, 
     main = "ROC curve –¥–ª—è –º–æ–¥–µ–ª–∏ —Å SMOTE", 
     col = "blue", 
     lwd = 2)

# Precision, Recall, F1 –¥–ª—è –º–æ–¥–µ–ª–∏ —Å SMOTE
TP_smote <- conf_matrix_smote$table[2, 2]
FP_smote <- conf_matrix_smote$table[1, 2]
TN_smote <- conf_matrix_smote$table[1, 1]
FN_smote <- conf_matrix_smote$table[2, 1]

precision_smote <- TP_smote / (TP_smote + FP_smote)
recall_smote <- TP_smote / (TP_smote + FN_smote)
f1_score_smote <- 2 * (precision_smote * recall_smote) / (precision_smote + recall_smote)

cat("Precision –¥–ª—è SMOTE: ", precision_smote, "\n")
cat("Recall –¥–ª—è SMOTE: ", recall_smote, "\n")
cat("F1 –¥–ª—è SMOTE: ", f1_score_smote, "\n")
```

```{r}

library(tidymodels)
library(themis)
library(caret)
library(pROC)

# –§–∏–∫—Å–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å
set.seed(123)

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ
n <- nrow(stroke_data3)
train_index <- sample(1:n, size = 0.8 * n)


# –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é Undersampling
train_data_under <- stroke_data3[train_index, ]
train_data_under_balanced <- ovun.sample(stroke ~ ., 
                                        data = train_data_under, 
                                        method = "under", 
                                        N = 2 * sum(train_data_under$stroke == 1))$data

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
model_under <- glm(stroke ~ ., 
                  data = train_data_under_balanced,
                  family = binomial)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
prob_predictions_under <- predict(model_under, 
                                  newdata = test_data_5, 
                                  type = "response")

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤ –∫–ª–∞—Å—Å—ã —Å –ø–æ—Ä–æ–≥–æ–º 0.75
class_predictions_under <- ifelse(prob_predictions_under > 0.5, 1, 0)

# –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –¥–ª—è –º–æ–¥–µ–ª–∏ —Å Undersampling
conf_matrix_under <- confusionMatrix(as.factor(class_predictions_under), 
                                     as.factor(test_data_5$stroke))

print(conf_matrix_under)

# ROC –∫—Ä–∏–≤–∞—è
roc_curve_under <- roc(test_data_5$stroke, prob_predictions_under)
auc_value_under <- auc(roc_curve_under)
cat("AUC –¥–ª—è –º–æ–¥–µ–ª–∏ —Å Undersampling: ", auc_value_under, "\n")
plot(roc_curve_under, 
     main = "ROC curve –¥–ª—è –º–æ–¥–µ–ª–∏ —Å Undersampling", 
     col = "green", 
     lwd = 2)

# Precision, Recall, F1 –¥–ª—è –º–æ–¥–µ–ª–∏ —Å Undersampling
TP_under <- conf_matrix_under$table[2, 2]
FP_under <- conf_matrix_under$table[1, 2]
TN_under <- conf_matrix_under$table[1, 1]
FN_under <- conf_matrix_under$table[2, 1]

precision_under <- TP_under / (TP_under + FP_under)
recall_under <- TP_under / (TP_under + FN_under)
f1_score_under <- 2 * (precision_under * recall_under) / (precision_under + recall_under)

cat("Precision –¥–ª—è Undersampling: ", precision_under, "\n")
cat("Recall –¥–ª—è Undersampling: ", recall_under, "\n")
cat("F1 –¥–ª—è Undersampling: ", f1_score_under, "\n")
```

Random forest: 

```{r}
install.packages("randomForest")
install.packages("caret")

library(randomForest)
library(caret)

# –§–∏–∫—Å–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å
set.seed(123)

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
n <- nrow(stroke_data3)
train_index <- sample(1:n, size = 0.8 * n)
train_data_4 <- stroke_data3[train_index, ]
test_data_4 <- stroke_data3[-train_index, ]

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Random Forest –±–µ–∑ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
rf_model <- randomForest(
  stroke ~ ., 
  data = train_data_4, 
  ntree = 1000,
  importance = TRUE
)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
rf_prob <- predict(rf_model, newdata = test_data_4, type = "prob")

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞ 0.5
rf_pred_class <- ifelse(rf_prob[, "1"] > 0.75, 1, 0)

# Confusion matrix
conf_matrix_rf <- confusionMatrix(
  as.factor(rf_pred_class), 
  as.factor(test_data_4$stroke),
  positive = "1"
)
print(conf_matrix_rf)

# –ú–µ—Ç—Ä–∏–∫–∏ –≤—Ä—É—á–Ω—É—é
TP2 <- conf_matrix_rf$table[2, 2]
FP2 <- conf_matrix_rf$table[1, 2]
TN2 <- conf_matrix_rf$table[1, 1]
FN2 <- conf_matrix_rf$table[2, 1]

precision2 <- TP2 / (TP2 + FP2)
recall2 <- TP2 / (TP2 + FN2)
f1_score2 <- 2 * (precision2 * recall2) / (precision2 + recall2)

cat("üéØ –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –ø–æ—Ä–æ–≥—É 0.5:\n")
cat("Precision: ", precision2, "\n")
cat("Recall: ", recall2, "\n")
cat("F1: ", f1_score2, "\n")

```
## Roc curve 
#roc_curve4 <- roc(test_data_4$stroke, rf_prob)
auc_value4 <- auc(roc_curve4)
print(paste("AUC: ", auc_value4))
plot(roc_curve4, main = "ROC curve for RF model", col = "blue", lwd = 2)

```{r}
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –Ω—É–∂–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤
install.packages("class")      # –î–ª—è KNN
install.packages("caret")      # –î–ª—è –º–µ—Ç—Ä–∏–∫ –∏ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
install.packages("ROSE")       # –î–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏

library(class)
library(caret)
library(ROSE)

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏
set.seed(123)

# üìå 1. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
n <- nrow(stroke_data3)
train_index <- sample(1:n, size = 0.8 * n)
train_data_5 <- stroke_data3[train_index, ]
test_data_5 <- stroke_data3[-train_index, ]

# üìå 2. –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏
train_balanced_5 <- ovun.sample(
  stroke ~ ., 
  data = train_data_5, 
  method = "over", 
  N = 2 * sum(train_data_5$stroke == 0)
)$data

# üìå 3. –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
train_y <- as.factor(train_balanced_5$stroke)
test_y  <- as.factor(test_data_5$stroke)

# üìå 4. –£–±–∏—Ä–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
train_x <- train_balanced_5[, setdiff(names(train_balanced_5), "stroke")]
test_x  <- test_data_5[, setdiff(names(test_data_5), "stroke")]

# üìå 5. –ü–æ–¥–±–æ—Ä –ª—É—á—à–µ–≥–æ k —Å –ø–æ–º–æ—â—å—é –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
# –û–ø—Ä–µ–¥–µ–ª–∏–º –¥–∏–∞–ø–∞–∑–æ–Ω k (–æ—Ç 1 –¥–æ 20)
k_values <- 1:20

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ F1-–º–µ—Ç—Ä–∏–∫–∏
f1_scores <- numeric(length(k_values))

# –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –∫–∞–∂–¥–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é k –∏ –≤—ã—á–∏—Å–ª—è–µ–º F1-–º–µ—Ç—Ä–∏–∫—É
for (k in k_values) {
  # –°—Ç—Ä–æ–∏–º –º–æ–¥–µ–ª—å KNN
  knn_predictions <- knn(
    train = train_x,
    test = test_x,
    cl = train_y,
    k = k
  )
  
  # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
  conf_matrix_knn <- confusionMatrix(knn_predictions, test_y, positive = "1")
  
  # –ò–∑–≤–ª–µ–∫–∞–µ–º TP, FP, TN, FN
  TP <- conf_matrix_knn$table[2, 2]
  FP <- conf_matrix_knn$table[1, 2]
  TN <- conf_matrix_knn$table[1, 1]
  FN <- conf_matrix_knn$table[2, 1]
  
  # –í—ã—á–∏—Å–ª—è–µ–º Precision, Recall, F1
  precision <- TP / (TP + FP)
  recall <- TP / (TP + FN)
  f1_score <- 2 * (precision * recall) / (precision + recall)
  
  # –°–æ—Ö—Ä–∞–Ω—è–µ–º F1 –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ k
  f1_scores[k] <- f1_score
}

# üìå 6. –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã F1 –¥–ª—è –∫–∞–∂–¥–æ–≥–æ k
cat("F1-–º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π k:\n")
print(data.frame(k = k_values, F1 = f1_scores))

# üìå 7. –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ k –ø–æ F1
best_k <- k_values[which.max(f1_scores)]
cat("\n–õ—É—á—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ k –ø–æ F1-–º–µ—Ç—Ä–∏–∫–µ:", best_k, "\n")
```




